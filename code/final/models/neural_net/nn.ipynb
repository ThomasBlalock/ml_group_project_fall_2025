{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b801301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchviz import make_dot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, confusion_matrix, classification_report \n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "target_dir = \"final\"\n",
    "max_depth = 10\n",
    "current_path = os.path.abspath(os.getcwd())\n",
    "start_path = current_path\n",
    "for i in range(max_depth):\n",
    "    current_dir_name = os.path.basename(current_path)\n",
    "    if current_dir_name == target_dir:\n",
    "        os.chdir(current_path)\n",
    "        break\n",
    "    parent_path = os.path.dirname(current_path)\n",
    "    current_path = parent_path\n",
    "\n",
    "DATA_FILE = \"data/nn_df.csv\"\n",
    "ARTIFACTS_DIR = \"models/neural_net/model_artifacts\"\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 100\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create artifacts directory\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# --- 1. Data Loading & Preprocessing ---\n",
    "\n",
    "def load_and_prep_data(filepath):\n",
    "    print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    numeric_cols = [\n",
    "        'MEDIAN_HOUSEHOLD_INCOME', 'POVERTY_RATE', 'UNEMPLOYMENT_RATE', \n",
    "        'SNAP_RECEIPT_RATE', 'POP_16_PLUS', 'HOUSEHOLDS_TOTAL'\n",
    "    ]\n",
    "\n",
    "    categorical_cols = [\n",
    "        'State', 'Low_Threshold_Type', 'last_year_cluster'\n",
    "    ]\n",
    "    \n",
    "    target_col = 'Food_Insecurity_Rate'\n",
    "    df_clean = df.dropna(subset=numeric_cols + categorical_cols + [target_col]).copy()\n",
    "    df_clean = df_clean[numeric_cols + categorical_cols + [target_col]].copy()\n",
    "    unique_values = {col: sorted(list(df_clean[col].unique().astype(str))) for col in categorical_cols}\n",
    "    for col in categorical_cols:\n",
    "        df_clean[col] = df_clean[col].astype(str)\n",
    "    df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
    "    feature_columns = [c for c in df_encoded.columns if c != target_col]\n",
    "    encoded_cat_cols = [c for c in feature_columns if c not in numeric_cols]\n",
    "    \n",
    "    print(f\"Total Features after Encoding: {len(feature_columns)}\")\n",
    "\n",
    "    # Split Data\n",
    "    X_numeric = df_encoded[numeric_cols].values\n",
    "    X_categorical = df_encoded[encoded_cat_cols].values.astype(float)\n",
    "    y = df_encoded[target_col].values\n",
    "\n",
    "    # Stratification\n",
    "    stratify_bins = pd.qcut(y, q=5, labels=False, duplicates='drop')\n",
    "\n",
    "    X_num_train, X_num_test, X_cat_train, X_cat_test, y_train, y_test = train_test_split(\n",
    "        X_numeric, X_categorical, y, \n",
    "        test_size=TEST_SIZE, \n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=stratify_bins\n",
    "    )\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
    "    X_num_test_scaled = scaler.transform(X_num_test)\n",
    "\n",
    "    X_train = np.hstack([X_num_train_scaled, X_cat_train])\n",
    "    X_test = np.hstack([X_num_test_scaled, X_cat_test])\n",
    "    \n",
    "    metadata = {\n",
    "        'numeric_cols': numeric_cols,\n",
    "        'categorical_cols': categorical_cols,\n",
    "        'encoded_cat_cols': encoded_cat_cols,\n",
    "        'feature_columns': feature_columns, \n",
    "        'unique_values': unique_values \n",
    "    }\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler, metadata\n",
    "\n",
    "# --- 2. Dataset & Model ---\n",
    "\n",
    "class FoodSecurityDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets).view(-1, 1)\n",
    "\n",
    "    def __len__(self): return len(self.features)\n",
    "    def __getitem__(self, idx): return self.features[idx], self.targets[idx]\n",
    "\n",
    "class FoodSecurityFFNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FoodSecurityFFNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.act3 = nn.ReLU()\n",
    "        \n",
    "        self.layer4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop1(self.act1(self.bn1(self.layer1(x))))\n",
    "        x = self.drop2(self.act2(self.bn2(self.layer2(x))))\n",
    "        x = self.act3(self.bn3(self.layer3(x)))\n",
    "        return self.layer4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9038e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading and Preprocessing Data ---\n",
      "Total Features after Encoding: 59\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 10/100 | Train: 0.0005 | Val: 0.0004\n",
      "Epoch 20/100 | Train: 0.0004 | Val: 0.0003\n",
      "Epoch 30/100 | Train: 0.0004 | Val: 0.0003\n",
      "Epoch 40/100 | Train: 0.0004 | Val: 0.0003\n",
      "Epoch 50/100 | Train: 0.0003 | Val: 0.0003\n",
      "Epoch 60/100 | Train: 0.0003 | Val: 0.0003\n",
      "Epoch 70/100 | Train: 0.0003 | Val: 0.0003\n",
      "Epoch 80/100 | Train: 0.0003 | Val: 0.0003\n",
      "Epoch 90/100 | Train: 0.0003 | Val: 0.0003\n",
      "Epoch 100/100 | Train: 0.0003 | Val: 0.0003\n",
      "\n",
      "Final R^2 Score on Test Set: 0.8407\n",
      "\n",
      "--- Calculating Confusion Matrix ---\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Pred_Low  Pred_Moderate  Pred_Elevated  Pred_High\n",
      "Actual_Low            593            205             24          3\n",
      "Actual_Moderate        97            581            187         12\n",
      "Actual_Elevated         2            177            615        117\n",
      "Actual_High             0             13            222        723\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Low       0.86      0.72      0.78       825\n",
      "    Moderate       0.60      0.66      0.63       877\n",
      "    Elevated       0.59      0.68      0.63       911\n",
      "        High       0.85      0.75      0.80       958\n",
      "\n",
      "    accuracy                           0.70      3571\n",
      "   macro avg       0.72      0.70      0.71      3571\n",
      "weighted avg       0.72      0.70      0.71      3571\n",
      "\n",
      "\n",
      "--- Saving Artifacts to 'models/neural_net/model_artifacts' ---\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Training & Saving ---\n",
    "\n",
    "def train_and_save():\n",
    "    X_train, X_test, y_train, y_test, scaler, metadata = load_and_prep_data(DATA_FILE)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        FoodSecurityDataset(X_train, y_train), \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        drop_last=True\n",
    "    )\n",
    "    test_loader = DataLoader(FoodSecurityDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FoodSecurityFFNN(input_dim).to(DEVICE)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train Loop\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(inputs), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                val_loss += criterion(model(inputs), targets).item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(test_loader.dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} | Train: {epoch_loss:.4f} | Val: {epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.FloatTensor(X_test).to(DEVICE)).cpu().numpy().flatten()\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"\\nFinal R^2 Score on Test Set: {r2:.4f}\")\n",
    "\n",
    "    # --- Confusion Matrix Calculation ---\n",
    "    print(\"\\n--- Calculating Confusion Matrix ---\")\n",
    "    bins = [0, 0.115, 0.138, 0.164, 1]\n",
    "    labels = [\"Low\", \"Moderate\", \"Elevated\", \"High\"]\n",
    "    preds_clipped = np.clip(preds, 0, 1)\n",
    "\n",
    "    y_test_cat = pd.cut(y_test, bins=bins, labels=labels, include_lowest=True)\n",
    "    preds_cat = pd.cut(preds_clipped, bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # Generate Matrix\n",
    "    cm = confusion_matrix(y_test_cat, preds_cat, labels=labels)\n",
    "    cm_df = pd.DataFrame(cm, index=[f\"Actual_{l}\" for l in labels], columns=[f\"Pred_{l}\" for l in labels])\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm_df)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    report_dict = classification_report(y_test_cat, preds_cat, labels=labels, output_dict=True)\n",
    "    print(classification_report(y_test_cat, preds_cat, labels=labels))\n",
    "\n",
    "    labels = [\"Low\", \"Moderate\", \"Elevated\", \"High\"]\n",
    "    export_data = {\n",
    "        \"r2_score\": r2,\n",
    "        \"labels\": labels,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"classification_report\": report_dict\n",
    "    }\n",
    "\n",
    "    metrics_path = os.path.join(ARTIFACTS_DIR, \"model_metrics.json\")\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(export_data, f, indent=4)\n",
    "    # -------------------------------------------\n",
    "\n",
    "    # Save\n",
    "    print(f\"\\n--- Saving Artifacts to '{ARTIFACTS_DIR}' ---\")\n",
    "    torch.save(model.state_dict(), os.path.join(ARTIFACTS_DIR, \"food_security_model.pth\"))\n",
    "    joblib.dump(scaler, os.path.join(ARTIFACTS_DIR, \"scaler.save\"))\n",
    "    joblib.dump(metadata, os.path.join(ARTIFACTS_DIR, \"model_metadata.save\"))\n",
    "    with open(os.path.join(ARTIFACTS_DIR, \"training_history.json\"), 'w') as f:\n",
    "        json.dump(history, f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ae1d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m labels = [\u001b[33m\"\u001b[39m\u001b[33mLow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mModerate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mElevated\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHigh\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m export_data = {\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mr2_score\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mr2\u001b[49m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: labels,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfusion_matrix\u001b[39m\u001b[33m\"\u001b[39m: cm.tolist(),\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclassification_report\u001b[39m\u001b[33m\"\u001b[39m: report_dict\n\u001b[32m      7\u001b[39m }\n\u001b[32m      9\u001b[39m metrics_path = os.path.join(ARTIFACTS_DIR, \u001b[33m\"\u001b[39m\u001b[33mmodel_metrics.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(metrics_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'r2' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
